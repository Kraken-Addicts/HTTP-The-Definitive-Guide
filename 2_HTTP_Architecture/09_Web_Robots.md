## 09장 :octopus: 웹 로봇

### 9.1 　  크롤러와 크롤링　 
#### 처음 - 9.1.4　 `daelee`
- 여기에
- 문제를 작성해주세요
<details>
<summary> <b> :page_facing_up: 답지 </b>  </summary>
<div markdown="1">
  
- 여기에
- 해설을 작성해주세요

</div>
</details>

#### 9.1.5 - 끝　 `secho`

1번

크롤링에서 어떤 URL을 방문했는지 판단하기 위해서는 복잡한 (____)를 사용할 필요가 있다.

<br>

2번

대규모 웹 크롤러가 방문한 URL을 관리하기 위해 사용되는 방법중에 하나인

느슨한 존재 비트맵은 URL간 충돌이 일어날 수 있다. 그 이유에 대해 서술하세요.

<br>


3번

대부분의 웹 로봇은 URL들을 표준형식으로 **정규화**함으로써 
같은 리소스를 가리키는 다른 URL을 미리 제거하려한다. 
해당하는 정규화방식을 고르세요.

1. 포트번호가 없으면 :80을 추가
2. 모든 %xx 이스케이핑문자를 대응되는 문자로 변환
   - (이스케이핑이란 HTML코드를 화면상에 띄우기 위해 사용되는 코드. 
   `<br/>`을 브라우저 상에 띄우려면 `&lt;br /&gt;` , &lt == <, %gt = >으로 사용함)
3. #태그제거

<br>

4번

웹 마스터인 `daelee`는 악의를 품고 크롤러들에게 함정을 빠트리려고 한다.

1. (___)링크를 이용해 로봇을 무한루프에 빠져들게 하려한다.

그래서  `daelee`는 `subdir`에다가  2(__)으로 향하는 링크를 걸고 사악한 웃음을 지어보였다.

/home   ㅡ index.html(home의 하위)

​    ㅡ   subdir(home의 하위)
<br>




5번

함정에 빠져버린 크롤러 설계자 `hylee`는 화가나서 중복을 피할 기법을 도입했다. 
다음 설명하는 기법의 명칭은?

1. (_ _ _ _): 로봇이 웹 사이트에서 일정 시간동안 가져올 수 있는 페이지 숫자를 제한한다.
2. (_ _발견) :(___)링크를 통한 순환과 그와 비슷한 오설정은 일정 패턴을 따르는 경향이 있어서, 
반복되는 구성요소를 가진 URL의 크롤링을 거절한다.



<details>
<summary> <b> :page_facing_up: 답지 </b>  </summary>
<div markdown="1">

1번

크롤링에서 어떤 URL을 방문했는지 판단하기 위해서는 복잡한 **(자료구조)**를 사용할 필요가 있다. 
-> URL은 굉장히 많기에 속도와 메모리사용에서 효과적인 자료구조를 사용해야한다.



2번

대규모 웹 크롤러가 방문한 URL을 관리하기 위해 사용되는 방법중에 하나인

느슨한 존재 비트맵은 URL간 충돌이 일어날 수 있다. 그 이유에 대해 서술하세요.

-> URL갯수는 잠재적으로 무한, 존재 비트는 유한하므로 같은 존재 비트에 두 URL이 매핑되어 충돌할 수 있다.



3번

대부분의 웹 로봇은 URL들을 표준형식으로 **정규화**함으로써 같은 리소스를 가리키는 다른 URL을 미리 제거하려한다. 
해당하는 정규화방식을 고르세요.

1. 포트번호가 없으면 :80을 추가
2. 모든 %xx 이스케이핑문자를 대응되는 문자로 변환
(이스케이핑이란 HTML코드를 화면상에 띄우기 위해 사용됨 `<br/>`을 브라우저 상에 띄우려면 `&lt;br /&gt;` , &lt = <, %gt = >으로 사용함)
3. #태그제거

=> 1,2,3번



4번

웹 마스터인 `daelee`는 악의를 품고 크롤러들에게 함정을 빠트리려고 한다.

1. (**심볼릭**)링크를 이용해 로봇을 무한루프에 빠져들게 하려한다.

그래서  `daelee`는 `subdir`에다가  2(**/home**)으로 향하는 링크를 걸고  사악한 웃음을 지어보였다.

/home   ㅡ index.html (home의 하위)

​    		ㅡ subdir (home의 하위)



5번

함정에 빠져버린 크롤러 설계자 `hylee`는 화가나서 중복을 피할 기법을 도입했다. 다음 설명하는 기법의 명칭은?

1. (**스로틀링**): 로봇이 웹 사이트에서 일정 시간동안 가져올 수 있는 페이지 숫자를 제한한다.
2. (**패턴**발견) :(**심볼릭**)링크를 통한 순환과 그와 비슷한 오설정은 일정 패턴을 따르는 경향이 있어서, 
반복되는 구성요소를 가진 URL의 크롤링을 거절한다.

</div>
</details>
<br>



### 9.2 　  로봇의 HTTP　 `jehong`
- 여기에
- 문제를 작성해주세요
<details>
<summary> <b> :page_facing_up: 답지 </b>  </summary>
<div markdown="1">
  
- 여기에
- 해설을 작성해주세요

</div>
</details>
<br>

### 9.3 　  부적절하게 동작하는 로봇들　 `taelee`
- 여기에
- 문제를 작성해주세요
<details>
<summary> <b> :page_facing_up: 답지 </b>  </summary>
<div markdown="1">
  
- 여기에
- 해설을 작성해주세요

</div>
</details>
<br>

### 9.4 　  로봇 차단하기　 `taelee`
- 여기에
- 문제를 작성해주세요
<details>
<summary> <b> :page_facing_up: 답지 </b>  </summary>
<div markdown="1">
  
- 여기에
- 해설을 작성해주세요

</div>
</details>
<br>

### 9.5 　  로봇 에티켓　 `yeosong`
1. 신원식별
- 로봇의 신원: HTTP `_________` 필드를 사용해서 웹 서버에게 로봇 이름을 말하라.
  로봇이 무엇을 하는지 이해를 돕기 위한 것으로, 로봇의 목적과 정책을 기술한 URL을 포함시키기도 한다.
- 기계의 신원: 웹 사이트가 로봇의 IP 주소를 호스트 명을 통해 `______`할 수 있도록 하라.
  이 로봇에 대해 책임이 있는 조직을 찾을 수 있도록!
- 연락처: HTTP 폼 필드를 사용해서 연락 가능한 `______`를 제공하라.

2. 동작
- 로봇이 노련해질때까지 365일 감시하라
- 대역폭 소비를 감시하고 조직에 로봇의 활동을 알려라. 
- 로깅을 철저히 하라
- 매번 조정하고 개선하라

3. 스스로를 제한하라
- URL을 필터링하여 관심없는 데이터를 걸러라.
  (예: `.Z, .gz, .tar, .zip` 등 압축파일이나 아카이브, `.exe` 같은 실행프로그램, 이미지 파일 등)
- `______`나 `____` 를 포함하는 동적 URL을 필터링 해라
- Accept 관련 헤더로 서버에게 어떤 콘텐츠를 이해할 수 있는지 알려라.
- robots.txt를 따르라
- 로봇이 특정 사이트에 너무 자주(수 초에 한번 이상) 접근하면 어떤 문제가 생길까요❓
- 로봇이 의도치 않은 루프에 빠지지 않게 하려면 어떻게 해야 할까요❓

4. 루프와 중복을 견뎌내기, 그리고 그 외의 문제들
- 모든 응답 코드 다루기
- URL 정규화 하기
- 적극적으로 순환 피하기
- 함정 감시하기
- 블랙리스트 관리하기

5. 확장성
- 풀고 있는 문제가 얼마나 큰지 미리 계산하라. 웹은 거대하다.
- 네트워크 대역폭의 실제 사용량을 측정하여 최적화에 반영하라.
- 얼마나 많은 시간이 필요한지 이해하고, 실제 소요된 시간과 추정의 차이를 확인하라.
- 대규모 크롤링의 경우 하드웨어, 프로세서 분할을 이해하고 사용하라.

6. 신뢰성
- 철저한 테스트로 성능과 예상 메모리 사용량을 확인하라
- 실패가 발생한 지점에서 재시작 가능하도록 `_______/______` 기능을 미리 설계하라.
- 실패해도 계속 동작할 수 있도록 설계하라

7. 소통
- 미리미리 문의(항의)에 응답할 수 있도록 준비해두라. 
- 이해도가 다양한 사람들이 항의할 수 있으므로, 설득할 생각 말고 로봇 차단 규칙 표준을 설명하고,
  경우에 따라 크롤러에서 제거하고 블랙리스트에 추가할 수 있도록 하라.
- 즉각 대응하라. 



<details>
<summary> <b> :page_facing_up: 답지 </b>  </summary>
<div markdown="1">
  
1. 신원식별
- 로봇의 신원: HTTP `User-Agent` 필드를 사용해서 웹 서버에게 로봇 이름을 말하라.
  로봇이 무엇을 하는지 이해를 돕기 위한 것으로, 로봇의 목적과 정책을 기술한 URL을 포함시키기도 한다.
- 기계의 신원: 웹 사이트가 로봇의 IP 주소를 호스트 명을 통해 `역방향 DNS`할 수 있도록 하라.
  이 로봇에 대해 책임이 있는 조직을 찾을 수 있도록.
- 연락처: HTTP 폼 필드를 사용해서 연락 가능한 `이메일 주소`를 제공하라.

2. 동작
- 로봇이 노련해질때까지 365일 감시하라
- 대역폭 소비를 감시하고 조직에 로봇의 활동을 알려라. 
- 로깅을 철저히 하라
- 매번 조정하고 개선하라

3. 스스로를 제한하라
- URL을 필터링하여 관심없는 데이터를 걸러라.
  (예: `.Z, .gz, .tar, .zip` 등 압축파일이나 아카이브, `.exe` 같은 실행프로그램, 이미지 파일 등)
- `cgi`나 `?` 를 포함하는 동적 URL을 필터링 해라
- Accept 관련 헤더로 서버에게 어떤 콘텐츠를 이해할 수 있는지 알려라.
- robots.txt를 따르라
- 너무 자주 접근하면 어떤 문제가 생길까요❓ 트래픽을 다 막아서 사이트 주인이 항의를 할 수 있음.
- 로봇이 의도치 않은 루프에 빠지지 않게 하려면 어떻게 해야 할까요❓ 한 사이트에 대한 총 접근 횟수를 제한해야한다.

4. 루프와 중복을 견뎌내기, 그리고 그 외의 문제들
- 모든 응답 코드 다루기
- URL 정규화 하기
- 적극적으로 순환 피하기
- 함정 감시하기
- 블랙리스트 관리하기

5. 확장성
- 풀고 있는 문제가 얼마나 큰지 미리 계산하라. 웹은 거대하다.
- 네트워크 대역폭의 실제 사용량을 측정하여 최적화에 반영하라.
- 얼마나 많은 시간이 필요한지 이해하고, 실제 소요된 시간과 추정의 차이를 확인하라.
- 대규모 크롤링의 경우 하드웨어, 프로세서 분할을 이해하고 사용하라.

6. 신뢰성
- 철저한 테스트로 성능과 예상 메모리 사용량을 확인하라
- 실패가 발생한 지점에서 재시작 가능하도록 `체크포인트/재시작` 기능을 미리 설계하라.
- 실패해도 계속 동작할 수 있도록 설계하라

7. 소통
- 미리미리 문의(항의)에 응답할 수 있도록 준비해두라. 
- 이해도가 다양한 사람들이 항의할 수 있으므로, 설득할 생각 말고 로봇 차단 규칙 표준을 설명하고,
  경우에 따라 크롤러에서 제거하고 블랙리스트에 추가할 수 있도록 하라.
- 즉각 대응하라. 

</div>
</details>
<br>

### 9.6 　  검색엔진　 `yeosong`

#### 9.6.1 넓게 생각하라
현재의 검색엔진에서 제공하는 웹 페이지의 양은 어마어마하다.<br>
0.5초 * 십억개 / ((60초 / 일) * (60분 / 시간) * (24시간 / 일))  = 5700일이 걸린다.<br>
이런 요청들을 `병렬적 / 선형적`으로 수행해야 하는 것은 명백하다.

#### 9.6.2 현대적인 검색엔진의 아키텍처
- 사용자가 `질의`를 보냄 - `게이트웨이`를 거쳐서 ---> `풀 텍스트 색인` <--- `크롤러`가 수집한 웹 페이지들을 보냄

#### 9.6.3 풀 텍스트 색인
풀 텍스트 색인은 단어 하나를 입력받아 그 단어를 포함하고 있는 문서를 즉각 알려줄 수 있는 데이터베이스다.
이 문서들은 단어에 대한 색인을 생성시키므로, 색인 생성 이후에는 문서별 포함 단어 검색의 처리 속도를 단축할 수 있다❓ (O ------- X)

#### 9.6.4 질의 보내기
- 사용자가 질의를 웹 검색엔진 게이트웨이로 보내는 방법은, HTTP `_____`이나 `______` 요청을 이용해서 게이트웨이로 보내는 식이다. 

#### 9.6.5 검색 결과를 정렬하고 보여주기
관련도 랭킹(relevancy ranking)은 검색엔진 검색 결과의 순위를 매기기 위한 과정으로,<br>
검색엔진은 각 결과가 어떤 알고리즘에 의해 설정되는지 정돈된 명세로 공개할 의무가 있다❓ (O ------- X)

#### 9.6.6 스푸핑
스푸핑은 무엇일까요? 스푸핑의 간단한 예를 말해보세요❓

<details>
<summary> <b> :page_facing_up: 답지 </b>  </summary>
<div markdown="1">
  
#### 9.6.1 넓게 생각하라
현재의 검색엔진에서 제공하는 웹 페이지의 양은 어마어마하다.<br>
0.5초 * 십억개 / ((60초 / 일) * (60분 / 시간) * (24시간 / 일))  = 5700일이 걸린다.<br>
이런 요청들을 `병렬적`으로 수행해야 하는 것은 명백하다.

#### 9.6.2 현대적인 검색엔진의 아키텍처
- 사용자가 `질의`를 보냄 - `게이트웨이` -> `풀 텍스트 색인` <- `크롤러`가 수집한 웹 페이지들을 보냄

#### 9.6.3 풀 텍스트 색인
풀 텍스트 색인은 단어 하나를 입력받아 그 단어를 포함하고 있는 문서를 즉각 알려줄 수 있는 데이터베이스다.
이 문서들은 단어에 대한 색인을 생성시키므로, 색인 생성 이후에는 문서별 포함 단어 검색의 처리 속도를 단축할 수 있다. (X)
- 색인 생성 후에는 단어별로 이미 색인이 생성된 것이므로 다시 검색을 할 필요는 없다. 281p

#### 9.6.4 질의 보내기
- 사용자가 질의를 웹 검색엔진 게이트웨이로 보내는 방법은, HTTP `GET`이나 `POST` 요청을 이용해서 게이트웨이로 보내는 식이다. 

#### 9.6.5 검색 결과를 정렬하고 보여주기
관련도 랭킹(relevancy ranking)은 검색엔진 검색 결과의 순위를 매기기 위한 과정으로,<br>
검색엔진은 각 결과가 어떤 알고리즘에 의해 설정되는지 정돈된 명세로 공개할 의무가 있다. (X)
- 가장 엄격하게 감춰진 비밀들이라고 한다.

#### 9.6.6 스푸핑
스푸핑은 무엇일까요? 스푸핑의 간단한 예를 말해보세요.
- [위키백과 스푸핑](https://ko.wikipedia.org/wiki/%EC%8A%A4%ED%91%B8%ED%95%91)
- 스푸핑은 속임을 통한 공격을 총칭한다. 본 책에서는 수많은 키워드만 나열한 가짜 페이지나, 특정 단어에 대한 가짜 페이지를 생성하는 게이트웨이 애플리케이션 등을 예로 들고 있다. 

</div>
</details>
<br>

<br>

### 🤓 추천하는 크롬 익스텐션
- grammarly https://chrome.google.com/webstore/search/grammarly?hl=ko
